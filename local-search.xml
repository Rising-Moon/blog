<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/blog/2022/02/25/hello-world/"/>
    <url>/blog/2022/02/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%9F%BA%E7%A1%80%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%9F%BA%E7%A1%80%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="基础纹理"><a href="#基础纹理" class="headerlink" title="基础纹理"></a>基础纹理</h1><ul><li>纹理最初的目的就是使用一张图片来控制模型的外观。使用<strong>纹理映射</strong>技术，我们可以把一张图“黏”在模型表面，逐<strong>纹素</strong>（纹素的名字是为了与像素作区分）地控制模型的颜色。  </li><li>在美术人员建模时，通常会在建模软件中利用纹理展开技术把<strong>纹理映射坐标</strong>存储到每个顶点上。纹理映射坐标定义了该顶点在纹理中对应的2D坐标。通常，这些坐标使用一个二维变量(u,v)来表示，其中u是横向坐标，而v是纵向坐标。因此，纹理坐标也被称为UV坐标。（原来展UV是这个意思）  </li><li>尽管纹理的大小可以是多种多样的，例如可以是256x256或者1028x1028的，但顶点的UV坐标通常都被归一化到[0,1]范围内。</li><li>注意OpenGL（原点位于左下）和DirectX（原点位于左上）中纹理坐标的不同。</li></ul><h2 id="纹理资源属性"><a href="#纹理资源属性" class="headerlink" title="纹理资源属性"></a>纹理资源属性</h2><h3 id="纹理类型。"><a href="#纹理类型。" class="headerlink" title="纹理类型。"></a>纹理类型。</h3><p>选择合适的类型是为了让unity知道我们的意图，为Unity Shader传递正确的纹理，并在一些情况下可以让Unity对该纹理进行优化。</p><h3 id="Wrap-Mode"><a href="#Wrap-Mode" class="headerlink" title="Wrap Mode"></a>Wrap Mode</h3><p>决定了当纹理坐标超过[0,1]范围后将会如何被平铺。Wrap Mode有两种模式：</p><ul><li>Repeat模式下，纹理坐标超过1，那它的整数部分就会被舍弃，而使用小数部分进行采样，这样的结果是纹理将会不断重复；</li><li>Clamp模式下，如果纹理坐标大于1，那么将会截取到1，如果小于0，那么将会截取到0。<h3 id="Filter-Mode"><a href="#Filter-Mode" class="headerlink" title="Filter Mode"></a>Filter Mode</h3></li><li>它决定了当纹理由于变换而产生拉伸时将会采用哪种滤波模式。Filter Mode支持3种模式：Point、Bilinear、Trilinear。它们得到的图片滤波效果依次上升，但需要耗费的性能也依次增大。纹理滤波会影响放大或缩小纹理得到的图片质量。例如把64X64大小的纹理贴在一个512X512大小的平面上时，就需要放大纹理。纹理缩小的过程比放大更加复杂，此时原纹理多个像素将会对应一个目标像素。纹理缩放更加复杂的原因在于我们往往需要处理锯齿问题，最常用的方法就是多级渐远纹理技术。多级渐远纹理技术是将原纹理提前使用滤波处理来得到很多更小的图像，形成一个图像金字塔，每一层都是对上一层图像降采样的结果。当物体原理摄像机时就可以直接使用较小的纹理。缺点是通常会多占用33%的内存空间。  </li><li>在Unity中，我们将纹理类型选择为Advanced，再勾选Generate Mip Maps即可开启多级渐远纹理技术。</li></ul><p>在内部实现上：</p><ul><li>Point模式使用<strong>最近邻</strong>滤波，在放大缩小时，它的采样像素数目通常只有一个，因此图像会看起来有种像素风格的效果。</li><li>Bilinear滤波则使用了线性滤波，对于每个目标像素，它会找到4个相邻像素，然后对它们进行线性插值混合后得到最终像素，因此图像看起来像被模糊了。</li><li>Trilinear滤波几乎和Bilinear一样，只是Trilinear还会在多级渐远纹理之间进行混合。</li></ul><h2 id="纹理最大尺寸和纹理模式"><a href="#纹理最大尺寸和纹理模式" class="headerlink" title="纹理最大尺寸和纹理模式"></a>纹理最大尺寸和纹理模式</h2><ul><li>Unity允许我们为不同目标平台选择不同的分辨率。<br>如果导入的纹理大小超过了Max Texture Size中设置的值，那么就会被缩放到这个值。导入的纹理可以是非正方形的，但长宽的大小应该是2的幂，如2,4,8,16,32,64,128,512,1024等，如果使用了非2的幂（NPOT）大小的纹理，那么这些纹理往往会占用更多的内存空间，而且GPU读取该纹理的速度也会下降，有些平台甚至不支持NPOT纹理。</li><li>Format决定了Unity内部使用哪种格式来存储该纹理。如果我们将Texture Type设置为Advanced，那么会有更多的Format供我们选择。这里不介绍每种纹理格式，但要知道纹理格式精度越高，越消耗性能，但是得到的效果也越好。</li></ul><h2 id="凹凸映射"><a href="#凹凸映射" class="headerlink" title="凹凸映射"></a>凹凸映射</h2><p>凹凸映射目的是使用一张纹理来修改模型表面的法线，一遍提供更多的细节。这只是让模型看起来好像是凹凸不平的，但可以从模型轮廓看出破绽。<br>目前有两种方法可以用来凹凸映射：</p><ul><li><p>使用<strong>高度纹理</strong>来模拟<strong>表面位移</strong>，然后得到一个修改后的法线值，这种方法被称作<strong>高度映射</strong></p></li><li><p>使用<strong>法线纹理</strong>来直接存储表面法线，这种方法被称作<strong>法线映射</strong></p><h3 id="高度纹理"><a href="#高度纹理" class="headerlink" title="高度纹理"></a>高度纹理</h3><p>使用一张高度图来实现凹凸映射。高度图中存储的是强度值，用于表示表面局部区域的海拔高度。因此颜色越前表示该位置的表面越向外凸起，而颜色越深越向里凹。这种方法好处是直观，可以明确知道一个模型表面的凹凸情况，但缺点是计算更加复杂，实时计算时不能直接得到表面法线，而是需要像素的灰度值计算而得，因此需要消耗更多的性能。<br>高度图通常会和法线映射一起使用，用于给出表面凹凸的额外信息。但通常会使用法线映射来修改光照。</p><h3 id="法线纹理"><a href="#法线纹理" class="headerlink" title="法线纹理"></a>法线纹理</h3><p>由于法线分量范围在[-1,1]，而像素的分量范围为[0,1]，因此我们需要做一个映射，通常使用的映射就是：<br><img src="https://img-blog.csdnimg.cn/2019041623265237.png" alt="向量映射法线公式"><br>反映射就是个逆函数</p></li><li><p>将修改后的模型空间中的表面法线存储在一张纹理中的，这种纹理被称为是<strong>模型空间的法线纹理</strong>。</p></li><li><p>然而通常我们会采用另一种纹理坐标，即模型顶点的切线空间来储存法线。对于模型的每个顶点，它都有一个属于自己的切线空间，原点为该顶点本身，z轴为顶点的法线方向，x轴是顶点的切线方向，y轴可以由法线和切线方向差积得到，被称为副法线或者副切线。这种纹理被称为是<strong>切线空间的法线纹理</strong>。</p></li><li><p>模型空间的法线纹理通常看起来是五颜六色的，而切线空间的法线纹理通常看起来是浅蓝色的</p><h4 id="模型空间储存法线的优点"><a href="#模型空间储存法线的优点" class="headerlink" title="模型空间储存法线的优点"></a>模型空间储存法线的优点</h4></li><li><p>实现简单，更加直观。甚至不需要模型原始的法线和切线等信息，也就是计算更少。生成也非常简单。</p></li><li><p>纹理坐标的缝合处和尖锐的边角部分，可见的突变（缝隙）较少，即可提供平滑边界。因为法线方向存储在同一坐标空间中，所以可以通过插值来平滑变换。</p><h4 id="切线空间储存法线的优点"><a href="#切线空间储存法线的优点" class="headerlink" title="切线空间储存法线的优点"></a>切线空间储存法线的优点</h4></li><li><p>自由度高。模型空间下的法线纹理是<strong>绝对法线信息</strong>，仅可用于创建它的那个模型，应用到其他模型上效果就完全错误了。而切线空间记录的是相对法线信息，这意味着可以把它应用到完全不同的网格上，也可以得到一个合理的结果。</p></li><li><p>可进行UV动画。比如，我们可以移动一个纹理的UV坐标来实现一个凹凸移动的效果，但是使用模型空间的法线纹理就会得到完全错误的结果。这种动画通常会使用在水或火山熔岩这类型的物体上。</p></li><li><p>可以重用法线纹理。比如一个砖块，用一张法线贴图就可以用到所有6个面上。</p></li><li><p>可压缩。由于切线空间下法线的Z方向总是正向，因此可以仅储存XY方向，而推导出Z方向。</p></li></ul><p>(顺手记录一个会用到的第四章的知识点，A坐标空间下，B坐标空间三条坐标轴的单位向量横向排列组成的矩阵就是A坐标空间到B坐标空间的变换矩阵，好像限制条件是没有缩放的情况下)</p><h2 id="Unity中的法线纹理类型"><a href="#Unity中的法线纹理类型" class="headerlink" title="Unity中的法线纹理类型"></a>Unity中的法线纹理类型</h2><p>当我们把法线纹理的纹理类型设置为Normal map时，可以使用Unity的内置函数UnpackNormal来得到正确的法线方向。<br>UnpackNormal的源码为：  </p><figure class="highlight nsis"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><pre><code class="hljs nsis">inline fixed3 Unpack<span class="hljs-params">Normal</span>DXT5nm(fixed4 packed<span class="hljs-params">normal</span>)&#123;<br>    fixed3 <span class="hljs-params">normal</span><span class="hljs-comment">;</span><br>    <span class="hljs-params">normal</span>.xy = packed<span class="hljs-params">normal</span>.wy * <span class="hljs-number">2</span> - <span class="hljs-number">1</span><span class="hljs-comment">;</span><br>    <span class="hljs-params">normal</span>.z = sqrt(<span class="hljs-number">1</span> - saturate(dot(<span class="hljs-params">normal</span>.xy,<span class="hljs-params">normal</span>.xy)))<span class="hljs-comment">;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-params">normal</span><span class="hljs-comment">;</span><br>&#125;<br><br>inline fixed3 Unpack<span class="hljs-params">Normal</span>(fixed4 packed<span class="hljs-params">normal</span>)&#123;<br>    <span class="hljs-comment">#if define(UNITY_NO_DXT5nm)</span><br>        <span class="hljs-keyword">return</span> packed<span class="hljs-params">normal</span>.xyz * <span class="hljs-number">2</span> - <span class="hljs-number">1</span><span class="hljs-comment">;</span><br>    <span class="hljs-comment">#else</span><br>        <span class="hljs-keyword">return</span> Unpack<span class="hljs-params">Normal</span>DXT5nm(packed<span class="hljs-params">normal</span>)<span class="hljs-comment">;</span><br>    <span class="hljs-comment">#endif</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>从代码可以看出，UnpackedNomal函数对使用DXT5nm压缩格式的法线纹理进行的相应解码方式。<br>DXT5nm压缩格式中，纹素的a通道（即w分量）对用了法线的x分量，g通道对应了法线的y分量，而纹理的r和b通道则会被舍弃，法线的z分量可以由xy分量推导得出。使用这种压缩可以减少法线纹理占用的空间。  </li><li>纹理设置中还有一个Create from Grayscale复选框，勾选这个复选框后，这张纹理贴图就被当做高度图来处理。勾选后会增加两个选项——Bumpiness和Filtering。其中Bumpiness用于控制凹凸程度，而Filtering用于控制我们使用哪种方式来计算凹凸程度。<br>一种是Smooth，这使得生成的法线纹理会比较平滑<br>另一种是sharp，它会使用Sobel滤波，来生成法线。Sobel滤波的实现非常简单，我们只需要在一个3x3的滤波器中计算x和y方向上的导数，然后从中得到法线即可。<h2 id="渐变纹理"><a href="#渐变纹理" class="headerlink" title="渐变纹理"></a>渐变纹理</h2>一开始时，人们使用纹理来定义一个物体的颜色，后来人们发现纹理其实可以用来储存任何表面属性。一种常见的用法就是使用渐变纹理来控制漫反射光照的结果。之前的满反射计算时，我们使用表面法线和光照方向的点积乘上物体的反照率来得到的。有时我们需要更灵活地控制光照结果。这种技术在《军团要塞2》中流行起来，也是由Valve公司提出来的。最初由Gooch等人在《A Non-Photorealistic Lighting Model For Automatic Technical Illustration》中被提出。这种技术可以使物体轮廓线更加明显，很多卡通风格的渲染中都使用了这种技术。</li></ul><h2 id="遮罩纹理"><a href="#遮罩纹理" class="headerlink" title="遮罩纹理"></a>遮罩纹理</h2><ul><li>遮罩允许我们保护某些区域，使它们免于某些修改。有时我们希望模型表面某些区域的反光强一些，某些区域弱一些，就可以使用遮罩纹理实现。</li><li>使用遮罩纹理的一般流程：通过采样得到遮罩纹理的纹素值，然后使用其中某个（或某几个）通道的值来与某种表面属性进行相乘，这样，当该通道的值为0时，可以保护表面不受该属性的影响。</li><li>在真实的游戏制作中，遮罩纹理已经不仅局限于保护某些区域使它们免于某些修改，而是可以存储任何我们希望逐像素控制的表面属性。通常一张纹理拥有RGBA四个通道，这意味这我们可以存储4个表面属性到一张纹理中。dota2中模型就有额外的两张纹理用来提供额外的8个表面属性。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>渲染流水线</title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%B8%B2%E6%9F%93%E6%B5%81%E6%B0%B4%E7%BA%BF%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%B8%B2%E6%9F%93%E6%B5%81%E6%B0%B4%E7%BA%BF%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="渲染流水线的起点是CPU"><a href="#渲染流水线的起点是CPU" class="headerlink" title="渲染流水线的起点是CPU"></a>渲染流水线的起点是CPU</h2><ul><li>应用程序阶段</li></ul><h3 id="把数据加载到显存中"><a href="#把数据加载到显存中" class="headerlink" title="把数据加载到显存中"></a>把数据加载到显存中</h3><ul><li>渲染所需数据从硬盘中加载到系统内存（RAM）中，又从系统内存中加载到显卡内存（VRAM）中。  </li><li>因为显卡对现存访问速度最快，并且大多数显卡对内存没有访问权限。  </li><li>数据加载到现存中后，系统内存中数据就可以删除，除非其中有数据会用作后续处理，例如网格数据用于碰撞检测则不进行删除。 因为从硬盘到内存的耗时较长，所以不会多次执行这个步骤。</li></ul><h3 id="设置渲染状态"><a href="#设置渲染状态" class="headerlink" title="设置渲染状态"></a>设置渲染状态</h3><ul><li>之后，开发者通过CPU设置渲染状态，从而指导GPU进行渲染。</li></ul><h3 id="调用Draw-Call"><a href="#调用Draw-Call" class="headerlink" title="调用Draw Call"></a>调用Draw Call</h3><ul><li>发起方是CPU，接收方是GPU。调用此方法只指向需要被渲染的图元列表。不再包涵任何材质信息，所有信息已经在前两步准备好了，GPU将信息整合计算并输出到屏幕上。</li></ul><h2 id="GPU流水线"><a href="#GPU流水线" class="headerlink" title="GPU流水线"></a>GPU流水线</h2><ul><li>几何概念阶段和光栅化概念阶段在GPU中执行</li></ul><h3 id="几何概念阶段"><a href="#几何概念阶段" class="headerlink" title="几何概念阶段"></a>几何概念阶段</h3><ul><li>顶点着色器 完全可编程</li><li>曲面细分着色器 可选着色器</li><li>几何着色器 可选着色器 可用于图元着色或者产生更多的图元</li><li>裁剪 可配置的 裁剪不再摄像头内的部分</li><li>屏幕映射 不可配置和编程 负责将每个图元的坐标转换到屏幕坐标上</li></ul><h3 id="光栅化概念阶段"><a href="#光栅化概念阶段" class="headerlink" title="光栅化概念阶段"></a>光栅化概念阶段</h3><ul><li>三角形设置阶段和三角形遍历阶段都是固定函数阶段。</li><li>片元着色器 完全可编程 实现逐片元着色</li><li>逐片元操作阶段 不可编程，具有很高配置性 修改颜色、深度缓冲、混合</li></ul><h3 id="详细解释"><a href="#详细解释" class="headerlink" title="详细解释"></a>详细解释</h3><h4 id="几何概念阶段："><a href="#几何概念阶段：" class="headerlink" title="几何概念阶段："></a>几何概念阶段：</h4><h4 id="顶点着色器"><a href="#顶点着色器" class="headerlink" title="顶点着色器"></a>顶点着色器</h4><ul><li>输入来自于CPU 处理单位为顶点 每次输入进来顶点都调用一次顶点着色器</li><li>其本身不可以创建或者销毁节点，也无法获得两个顶点之间的关系，这样的相互独立性，使GPU可以并行化处理每一个顶点。</li><li>主要工作：坐标变换和逐顶点光照。输出后续工作需要的数据</li><li>坐标变换：将顶点坐标从模型空间转换到齐次剪切空间。</li></ul><h4 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h4><ul><li>场景大小通常比摄像机可视范围大，将图元与摄像机的关系进行划分：完全在视野内、完全不在视野内、部分在视野内。</li><li>完全在视野内的传递到下一阶段。</li><li>完全不在视野内的不向下传递。</li><li>部分在视野内的，在分割处创建新的顶点，与视野内部分组成新图元传递到下一阶段，视野外部分不传递到下一阶段。</li></ul><h4 id="屏幕映射"><a href="#屏幕映射" class="headerlink" title="屏幕映射"></a>屏幕映射</h4><ul><li>将图元的x，y坐标转换为二维的屏幕坐标系中。</li><li>z坐标不作处理，z坐标与屏幕坐标系组成新的坐标系——窗口坐标系。</li><li>这些值传递到光栅化阶段。</li><li>ps：OpenGL中，屏幕左下角为最小坐标值，DirectX中，屏幕左上角为最小坐标值。</li></ul><h4 id="光栅化概念阶段："><a href="#光栅化概念阶段：" class="headerlink" title="光栅化概念阶段："></a>光栅化概念阶段：</h4><p>上一阶段输出信息：<br>顶点位置以及和它们相关的额外信息（例如：深度值即z坐标，法线方向，视角方向）<br>光栅化阶段重要目标：计算每个图元覆盖了哪些像素。</p><h4 id="三角形设置"><a href="#三角形设置" class="headerlink" title="三角形设置"></a>三角形设置</h4><ul><li>计算光栅化一个三角网格所需的信息。</li><li>计算每条边上的像素坐标，得出三角形边的表示方法。</li><li>输出为了下一个阶段做准备。</li></ul><h4 id="三角形遍历"><a href="#三角形遍历" class="headerlink" title="三角形遍历"></a>三角形遍历</h4><ul><li>检查每个像素是否被一个三角形网格覆盖，覆盖则生成一个片元。</li><li>输出一个片元序列。</li><li>ps：片元不等于像素，片元是一个包含了很多状态的集合，这些状态用于计算每个像素的最终颜色。</li></ul><h4 id="片元着色器（Directx中：像素着色器）"><a href="#片元着色器（Directx中：像素着色器）" class="headerlink" title="片元着色器（Directx中：像素着色器）"></a>片元着色器（Directx中：像素着色器）</h4><ul><li>另一个非常重要的可编程阶段。</li><li>完成很多重要的渲染操作，最重要的就是纹理采样。</li><li>输出一个或者多个颜色值</li><li>仅影响单个片元</li></ul><h4 id="逐片元操作（DirectX中：输出合并阶段）"><a href="#逐片元操作（DirectX中：输出合并阶段）" class="headerlink" title="逐片元操作（DirectX中：输出合并阶段）"></a>逐片元操作（DirectX中：输出合并阶段）</h4><ul><li>决定片元可见性</li><li>如果片元通过了所有测试，需要把这个片元的颜色值和已储存在颜色缓冲区的颜色合并。</li><li>高度可配置性指的是我们可以设置每一步的操作细节。</li><li>片元测试：<br>片元-&gt;模板测试-&gt;深度测试-&gt;混合-&gt;颜色缓冲区</li><li>模板测试：<br>读取该片元位置的模板值，然后将该值和读取到的参考值作比较，比较函数可以由开发者指定，如果没有通过测试，这个片元便会被舍弃，无论有没有通过测试，都可以根据结果以及之后深度测试的结果修改模板缓冲区。修改操作也由开发者指定。</li><li>深度测试：<br>将该片元的深度值与已经存在于深度缓冲区中的深度值作比较，比较函数也可以由开发者指定，通常为小于等于，以为我们总希望距离摄像机比较近的物体被渲染。如果这个片元没有通过测试，则被舍弃，并且没有更改深度缓冲区的权利。</li><li>合并：<br>在经过多次测试后，得到的值就要与上次绘制后颜色缓冲区的值合并，是覆盖之前的颜色还是其他处理就是合并操作需要处理的。</li><li>深度测试在片元着色器之前</li><li>光栅化时，没处理完一个片元就会对颜色缓冲区进行修改，即用户会看到不连贯的图像。所以GPU使用的双重缓冲，在后置缓冲中渲染场景，渲染结束后，与前值缓冲进行交换，避免了不连贯性。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>《UnityShader入门精要》</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E6%B3%95%E7%BA%BF%E5%92%8C%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E6%B3%95%E7%BA%BF%E5%92%8C%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="使用深度法线和纹理"><a href="#使用深度法线和纹理" class="headerlink" title="使用深度法线和纹理"></a>使用深度法线和纹理</h1><h2 id="获取深度和法线纹理"><a href="#获取深度和法线纹理" class="headerlink" title="获取深度和法线纹理"></a>获取深度和法线纹理</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h4 id="深度纹理"><a href="#深度纹理" class="headerlink" title="深度纹理"></a>深度纹理</h4><ul><li>就是一张渲染纹理，存储的像素值是高精度的深度值。</li><li>范围是[0,1]，通常是非线性分布的。</li><li>深度值来自于顶点变换后得到的归一化的设备坐标</li><li>顶点转换到齐次裁剪空间后，z坐标就是它的深度值。但是z分量的范围是[-1,1]所以我们为了能将其存储在图像中，使用：<br>d &#x3D; 0.5*z + 0.5<br>将其变为[0,1]范围中。</li><li>Unity中深度纹理可以来自于真正的深度缓存，也可以由单独一个Pass渲染获得，这取决于渲染路径和硬件。</li><li>使用延迟渲染路径可以直接从G-buffer中获得深度信息。</li><li>当无法直接获取深度缓存时，深度和法线纹理是通过一个单独的Pass渲染获得。</li><li>具体实现是，Unity会使用<strong>着色器替换</strong>技术选择渲染类型为Opaque的物体，判断它使用的渲染队列是否小于2500（包括Background、Geometry和AlphaTest），如果满足条件就把他渲染到深度和法线纹理中。</li><li>在Unity中，我们可以选择让一个摄像机生成一张深度纹理或是一张深度+法线纹理。</li><li>选择前者时，Unity会直接获得深度缓存或者按之前讲到的着色器替换技术，使用物体投射阴影时使用的Pass来获取深度纹理。如果shader中不包含这样的pass，那么物体就不会出现在深度纹理中。深度纹理的精度通常是24位或者16位，这取决于深度缓存的精度。</li><li>如果选择生成一张深度+法线纹理，Unity会创建一张和屏幕分辨率相同、精度为32位的纹理，其中观察空间下的法线信息会被编码进纹理的R和G通道，深度信息会被编码进B和A通道。法线信息在延迟渲染路径下很容易获取，将法线和深度缓存合并即可。在前向渲染中，默认情况下不会创建法线缓存，因此Unity底层使用一个单独的Pass把整个场景再次渲染一遍来完成。这个Pass被包含在Unity内置的一个Unity Shader中，可以在builtin_shaders-xxx&#x2F;DefaultResources&#x2F;Camera-DepthNormalTexture.shader文件中找到这个用于渲染深度和法线信息的Pass。</li></ul><h3 id="如何获取"><a href="#如何获取" class="headerlink" title="如何获取"></a>如何获取</h3><ul><li>获取深度纹理十分简单，在脚本中设置摄像机的depthTextureMode来完成：<figure class="highlight ini"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">camera.depthTextureMode</span> = DepthTextureMode.Depth<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure></li><li>之后在shader中声明_CameraDepthTexture变量来访问它。</li><li>同理，获取深度+法线纹理，我们只需要在代码中设置：<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">camera.depthTextureMode</span> = DepthTextureMode.DepthNormals<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure></li><li>之后在shader中声明_CameraDepthNormalTexture变量来访问它。</li><li>通常情况下，我们直接使用tex2D对纹理进行采样即可，但在某些平台上（例如PS3和PS2），我们需要一些特殊处理。Unity为我们提供了一个统一的宏，SAMPLE_DEPTH_TEXTURE来处理这些平台差异。<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-built_in">float</span> d = <span class="hljs-constructor">SAMPLE_DEPTH_TEXTURE(<span class="hljs-params">_CameraDepthTexture</span>,<span class="hljs-params">i</span>.<span class="hljs-params">uv</span>)</span>;<br></code></pre></td></tr></table></figure></li><li>i.uv是一个float2类型的变量，对应了当前像素的纹理坐标。</li><li>类似的宏还有SAMPLE_DEPTH_TEXTURE_PROJ，同样接受两个参数，第二个参数是一个float3或者float4类型的纹理坐标，前两个分量是uv坐标，会使用前两个分量除以第三个分量再进行采样，如果提供第四个分量，会再进行一次比较，通常用于阴影的实现中。<br>例如：<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-built_in">float</span> d = <span class="hljs-constructor">SAMPLE_DEPTH_TEXTURE_PROJ(<span class="hljs-params">_CameraDepthTexture</span>,UNITY_PROJ_COORD(<span class="hljs-params">i</span>.<span class="hljs-params">srcPos</span>)</span>);<br></code></pre></td></tr></table></figure></li><li>i.srcPos是顶点着色器中通过调用ComputerScreenPos(o.pos)得到的屏幕坐标。</li><li>取样得到的z分量是非线性的，我们需要把它转换到线性空间中，例如视角空间。</li><li>齐次裁剪空间转换到视角空间的方法可以由视角空间到齐次裁剪空间的公式逆推得到。</li><li>Unity提供了两个辅助函数来为我们进行上述的计算过过程：LinearEyeDepth和Linear01Depth。</li><li><strong>LinearEyeDepth</strong>负责把采样获得的深度值转换到观察空间下的深度值</li><li><strong>Linear01Dept</strong>h则会返回一个范围在[0,1]的线性深度值。</li><li>两个函数使用了内置变量_ZBufferParams变量来得到远近裁剪平面的距离。</li><li>如果我们需要获得深度+法线纹理，可以tex2D函数对_CameraDepthNormalTexture进行采样。</li><li>Unity提供了辅助函数DecodeDeothNormal对采样结果进行解码：<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">inline void <span class="hljs-constructor">DecodeDepthNormal(<span class="hljs-params">float4</span> <span class="hljs-params">enc</span>,<span class="hljs-params">out</span> <span class="hljs-params">float</span> <span class="hljs-params">depth</span>,<span class="hljs-params">out</span> <span class="hljs-params">float3</span> <span class="hljs-params">normal</span>)</span>&#123;<br>    depth = <span class="hljs-constructor">DecodeFloatRG(<span class="hljs-params">enc</span>.<span class="hljs-params">zw</span>)</span>;<br>    normal = <span class="hljs-constructor">DecodeViewNormalStereo(<span class="hljs-params">enc</span>)</span>;<br>&#125;<br></code></pre></td></tr></table></figure></li><li>第一个参数是对深度法线纹理的采样结果，这个结果是Unity对深度法线信息进行了编码后的结果，xy分量存储的是视角空间下的法线信息，而深度信息被编码进了zw分量。调用辅助函数后就可以得到解码后的深度和法线信息了。这个深度值是范围在[0,1]的线性深度值（与单独的深度纹理中存储的深度值不同），得到的法线则是视角空间下的法线方向。同样，我们也可以通过调用DecodeFloatRG和DecodeViewNormalStereo来解码深度+法线纹理中的深度和法线信息。</li></ul><h3 id="查看深度和法线纹理"><a href="#查看深度和法线纹理" class="headerlink" title="查看深度和法线纹理"></a>查看深度和法线纹理</h3><ul><li>在设置了摄像机渲染深度和法线纹理后可以在Frame Debug中查看。</li><li>单独深度纹理展示的是非线性空间下的深度值；深度加法线纹理展示的是编码以后的结果。</li><li>我们可以编写shader来显示线性空间或者解码后的结果。</li></ul><h2 id="再谈运动模糊"><a href="#再谈运动模糊" class="headerlink" title="再谈运动模糊"></a>再谈运动模糊</h2><ul><li>运动模糊更广泛的技术是使用速度映射图。速度映射图存储了每个像素的速度，然后使用这个速度来决定模糊的大小和方向。</li><li>速度缓冲生成的方法很多。一种是把场景中所有物体速度渲染到一张纹理中。这种方法缺点是在于需要修改场景中所有物体的shader代码，使其添加计算速度的代码并输出到一个渲染纹理中。</li><li>《GPU Gems3》中介绍了一种生成速度映射图的方法。这种方法利用深度纹理在片元着色器中为每个像素计算其在世界空间下的位置，通过使用当前视角*投影矩阵的逆矩阵对NDC下的顶点坐标进行变换得到的。用相同方法得到前一帧中该位置的NDC坐标。然后对其求差值，生成该像素的速度。优点是在屏幕后处理时完成了整个效果的模拟；缺点是需要在片元着色器中进行两次矩阵乘法的操作，对性能有所影响。</li></ul><h2 id="全局雾效"><a href="#全局雾效" class="headerlink" title="全局雾效"></a>全局雾效</h2><ul><li><strong>雾效</strong>是游戏里经常使用的一种效果。Unity内置的雾效可以产生基于距离的线性或指数雾效。想要在自己编写的顶点&#x2F;片元着色器中实现这些雾效，我们需要在Shader中添加#pragma multi_compile_fog指令，同时还需要使用相关内置宏，例如UNITY_FOG_COORDS、UNITY_TRANSFER_FOG 和 UNITY_APPLY_FOG等。</li><li>这种方法的缺点在于，我们不仅需要为场景中的所有物体添加相关渲染代码，而且能够给实现的效果也非常有限。</li><li>我们将学习一种基于屏幕后处理的全局雾效。这种方法可以方便地模拟各种雾效。</li><li>基于屏幕后处理的全局雾效的关键是，根据深度纹理来重建每个像素在世界空间下的位置。</li><li>重建方法：（比运动模糊中的更效率）对图像空间下的视锥体射线（从摄像机出发，指向图像上某点的射线）进行插值，这条射线存储了该像素在世界空间下到摄像机的方向信息。然后我们把该射线和线性化后的视角空间下的深度值相乘，再加上摄像机的世界位置，就可以得到该像素在世界空间下的位置。当我们得到世界坐标后，就可以使用各个公式来模拟全局雾效了。</li></ul><h2 id="再谈边缘检测"><a href="#再谈边缘检测" class="headerlink" title="再谈边缘检测"></a>再谈边缘检测</h2><ul><li>之前我们使用的是Sobel算子，根据颜色来进行边缘检测实现描边效果。但是会得到许多我们不希望得到的边缘线。</li><li>我们将要使用深度和法线纹理进行边缘检测。</li><li>本节会使用Roberts算子进行边缘检测。</li><li>通过纹理采样，获取比较当前片元的左上与右下、左下与右上的片元的法线与深度值的相差大小，如果高于某个阈值，则认为这是一条边。这样的方法相对基于颜色的边缘检测更加精确。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E5%B1%8F%E5%B9%95%E5%90%8E%E5%A4%84%E7%90%86%E6%95%88%E6%9E%9C%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E5%B1%8F%E5%B9%95%E5%90%8E%E5%A4%84%E7%90%86%E6%95%88%E6%9E%9C%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="屏幕后处理效果"><a href="#屏幕后处理效果" class="headerlink" title="屏幕后处理效果"></a>屏幕后处理效果</h1><h2 id="建立一个基本的屏幕后处理脚本系统"><a href="#建立一个基本的屏幕后处理脚本系统" class="headerlink" title="建立一个基本的屏幕后处理脚本系统"></a>建立一个基本的屏幕后处理脚本系统</h2><ul><li>屏幕后处理，通常指的是在渲染完整个场景得到屏幕图像后，再对这个图像进行一系列操作，实现各种屏幕特效。可以为游戏画面添加更多的艺术效果，例如景深、运动模糊。</li><li>要实现屏幕后处理的基础在于得到渲染后的屏幕图像吗，即抓取平屏幕，Unity为我们提供了这样一个函数：</li></ul><p><strong>MonoBehaviour.OnRenderImage(RenderTexture src,RenderTexture dest)</strong>  </p><ul><li>声明此函数后，unity会将屏幕图像存储在第一个参数对应的源渲染纹理中，通过函数一系列操作后，再把第二个参数对应的渲染纹理显示到屏幕上。</li><li>我们通常是利用Graphics.Blit函数来完成对渲染纹理的处理，函数的3中声明：<br>public static void Blit(Texture src,RenderTexture dest);<br>public static void Blit(Texture src,RenderTexture dest,Material mat,int pass &#x3D; -1);<br>public static void Blit(Texture src,Material,int pass &#x3D; -1);  </li><li>src:原纹理，通常是当前屏幕的渲染纹理或者上一步处理后得到的渲染纹理。</li><li>dst:目标渲染纹理，值为null就会直接渲染到屏幕上。</li><li>mat:是我们使用的材质，这个材质使用的Unity Shader将会进行各种屏幕后处理操作，src会被传递给Shader中名为_MainTex的纹理属性。</li><li>pass:默认为-1，表示会依次调用shader中的所有pass。否则调用指定索引的pass。</li><li>默认情况下，OnRenderImage会在所有透明、不透明物体渲染之后调用，以便渲染所有物体</li><li>通常后处理的步骤：</li></ul><ol><li>在摄像机中添加一个屏幕后处理脚本。</li><li>脚本中实现OnRenderImage函数来获取当前的屏幕渲染纹理。</li><li>调用Graphics.Blit函数使用特定的Unity Shader来对当前图像进行处理，再将返回的渲染纹理显示到屏幕上。</li><li>一些复杂的屏幕特效可能多次调用Graphics.Blit函数对上一步的输出结果进行下一次处理。</li></ol><ul><li>屏幕后处理之前，需要进行检查。例如当前平台是否支持纹理渲染和屏幕特效，或者是否支持使用Unity Shader。因此我们要创建一个用于屏幕后处理效果的基类。</li></ul><h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><ul><li>边缘检测的原理是利用一些边缘检测算子对图像进行<strong>卷积</strong>操作。</li><li>图像处理中，卷积操作指的就是使用一个<strong>卷积核</strong>对一张图像中的每个像素进行一系列操作。卷积核通常是一个四方形网格结构，该区域每个方格都有一个权重值。当对图像中某个像素进行卷积时，我们会把卷积核的中心放置于这个像素上，翻转核之后再依次计算核中每个元素和其覆盖的图像像素值的乘积并求和，得到的结果就是该位置的新像素值。</li></ul><h3 id="常见的边缘检测算子"><a href="#常见的边缘检测算子" class="headerlink" title="常见的边缘检测算子"></a>常见的边缘检测算子</h3><ul><li>用于边缘检测的卷积核也被称为边缘检测算子。</li><li>如果相邻像素之间存在差别明显的颜色、亮度、纹理等属性，我们就会认为它们之间应该有一条边界。这种相邻像素之间的插值可以用<strong>梯度</strong>来表示。<br><img src="https://img-blog.csdnimg.cn/20190502224814967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDg2NjM5,size_16,color_FFFFFF,t_70" alt="边缘检测算子">  </li><li>它们都包含了两个方向的卷积核，分别用于检测水平方向和竖直方向上的边缘信息。在进行边缘检测时，我们需要对每个像素进行一次卷积计算，得到两个方向上的梯度值Gx和Gy，而整体的梯度按照下面公式计算：<br><img src="https://img-blog.csdnimg.cn/20190502225140970.png" alt="梯度值计算公式"></li></ul><h2 id="高斯模糊"><a href="#高斯模糊" class="headerlink" title="高斯模糊"></a>高斯模糊</h2><ul><li>卷积的另一个常见应用就是高斯模糊。模糊实现的方法有很多，例如均值模糊和中值模糊。</li><li>均值模糊：卷积核中的各个元素都相等，并且相加等于1，也就是卷积后的像素值是其邻域内像素值得和。</li><li>中值模糊：选择邻域内对所有像素排序后的中值替换原颜色。</li><li>高斯模糊是一个更高级的模糊方法。<h3 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h3></li><li>高斯模糊使用的卷积核名为高斯核。</li><li>高斯核是一个正方形大小的滤波核，其中每个元素都基于下面的高斯方程：<br><img src="https://img-blog.csdnimg.cn/20190503205631803.png" alt="高斯方程">  </li><li>σ是标准方差（一般取1）</li><li>x和y分别对应了当前位置到卷积核中心的整数距离。</li><li>我们需要对高斯核中的权重进行归一化，这样图像才不会变暗。因此e前面的系数不会对实际结果有任何影响。</li><li>高斯方程很好地模拟了邻域每个像素对当前像素的影响程度——距离越近，影响越大。高斯核的维数越高，模糊程度越大。使用NxN的高斯核，就需要NxNxWxH（W和H为图像的宽和高）次纹理采样。</li><li>但我们可以把二维的高斯核拆成两个一维的高斯核，得到的结果是一样的，采样次数大大减少。</li><li>我们会使用先后两个Pass，第一个Pass将会使用竖直方向的一维高斯核对图像进行滤波，第二个Pass再使用水平方向的一维高斯核对图像进行滤波，得到最终的目标图像。</li><li>在实现中，我们还将利用图像缩放进一步提高性能，并通过高斯滤波的应用次数来控制模糊程度。（次数越多图像越模糊）</li></ul><h2 id="Bloom效果"><a href="#Bloom效果" class="headerlink" title="Bloom效果"></a>Bloom效果</h2><ul><li>游戏中一种常见特效</li><li>模拟真实摄像机的一种图像效果，让画面中比较亮的区域“扩散”到周围的区域中，造成朦胧的效果。</li><li>原理：首先根据一个阈值提取出图像中的较亮区域，把它们存储在一张渲染纹理中，利用高斯模糊对这张渲染纹理进行模糊处理，模拟光线的扩散效果，最后将其与原图混合，得到最终效果。</li></ul><h2 id="运动模糊"><a href="#运动模糊" class="headerlink" title="运动模糊"></a>运动模糊</h2><ul><li>模拟真实世界中的摄像机的一种效果。如果在摄像机曝光时，拍摄场景发生了变化，就会产生模糊画面。</li><li>运动模糊效果可以让物体运动看起来更加真实平滑。</li><li>运动模糊有多种实现方法：</li><li>一种实现方法是利用一块<strong>累计缓存</strong>来混合多张连续的图像。物体快速移动产生多张图像后，我们取它们之间的平均值作为最后的运动模糊图像。缺点是性能消耗大，想要获取多张帧图像意味着需要在同一帧里渲染多次场景。</li><li>另一种广泛的方法是使用<strong>速度缓存</strong>，这个缓存中存储了各个像素当前的运动速度，然后利用该值来决定模糊的方向和大小。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E5%99%AA%E5%A3%B0%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E5%99%AA%E5%A3%B0%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="使用噪声"><a href="#使用噪声" class="headerlink" title="使用噪声"></a>使用噪声</h1><h2 id="消融效果"><a href="#消融效果" class="headerlink" title="消融效果"></a>消融效果</h2><ul><li><strong>消融</strong>效果常见于游戏中的角色死亡、地图烧毁等效果。</li><li>原理十分简单，概括来说就是使用噪声纹理+透明度测试。我们使用噪声纹理采样的结果与某个阈值比较，如果小于阈值，就是用clip函数把对应的像素减掉，这些部分就对应了被“烧毁”的区域。镂空区域边缘的烧焦效果是将两种颜色混合，再用pow函数处理后，与原纹理颜色混合后的结果。</li><li>使用不同的噪声和纹理属性（材质面板上的Tiling和Offset值）都会得到不同的消融效果。想要得到好的消融效果，也需要美术人员提供合适的噪声纹理来配合。</li></ul><h2 id="水波效果"><a href="#水波效果" class="headerlink" title="水波效果"></a>水波效果</h2><ul><li>模拟实时水面时，我们也会使用噪声纹理。此时噪声通常会作为一个高度图，不断修改水面的法线方向。为了模拟水不断流动的效果，还会使用和时间相关的变量对噪声纹理进行采样，得到法线信息后，再进行正常的反射+折射计算，得到最后的水面波动效果。</li><li></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E9%9D%9E%E7%9C%9F%E5%AE%9E%E6%84%9F%E6%B8%B2%E6%9F%93%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E9%9D%9E%E7%9C%9F%E5%AE%9E%E6%84%9F%E6%B8%B2%E6%9F%93%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="非真实感渲染"><a href="#非真实感渲染" class="headerlink" title="非真实感渲染"></a>非真实感渲染</h1><ul><li>尽管游戏渲染一般都是以<strong>照相写实主义</strong>作为主要目标。但也有许多游戏使用了<strong>非真实感渲染</strong>的方法来渲染游戏画面。</li><li>非真实感渲染的目标是使用一些渲染方法，使得画面达到和某些特殊的绘画风格相似的效果。例如：卡通、水彩风格。</li></ul><h2 id="卡通风格的渲染"><a href="#卡通风格的渲染" class="headerlink" title="卡通风格的渲染"></a>卡通风格的渲染</h2><ul><li>实现卡通渲染有很多方法，其中之一就是基于色调的光照模型。</li><li>在实现中，我们往往会使用一张一维纹理进行采样，以控制漫反射的色调。</li><li>卡通风格的高光效果也和我们之前学习的光照不同，卡通风格中，模型的高光往往是一块块分界明显的纯色区域。</li><li>卡通风格通常还需要在物体边缘部分绘制轮廓</li></ul><h3 id="渲染轮廓线"><a href="#渲染轮廓线" class="headerlink" title="渲染轮廓线"></a>渲染轮廓线</h3><p>有许多轮廓线的渲染方法被提出，这里介绍其中五种：</p><ul><li>基于观察角度和表面法线的轮廓线渲染。使用视角方向和表面法线的点乘结果来得到轮廓线的信息。这种方法简单快速，一个Pass中就可以得到渲染结果，但局限性很大，很多模型渲染出来的描边效果不尽如人意。</li><li>过程式几何轮廓线渲染。使用两个Pass渲染，第一个Pass渲染背面的面片，并使用某些技术让它的轮廓可见；第二个Pass再正常渲染正面的面片。这种方法有点在于快速有效，并且适用于绝大多数表面光滑的模型，但它的缺点是不适合类似正方体这类表面平整的模型。</li><li>基于图像处理的轮廓线渲染。之前边缘检测的方法就属于这个类别。这种方法有点在于可以适用于任何类型的模型。但也有局限性，一些深度和法线变化很小的轮廓无法被检测出来，例如桌子上的纸张。</li><li>基于轮廓边检测的轮廓线渲染。上述提到的方法无法控制轮廓线的风格渲染。我们检测一条边是否是轮廓边的公式很简单：<figure class="highlight gcode"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs gcode"><span class="hljs-comment">(n0·v&gt;0)</span> ≠ <span class="hljs-comment">(n1·v&gt;0)</span><br></code></pre></td></tr></table></figure></li><li>其中n0和n1是相邻两个三角面片的法向量，v是视角到该边上任意顶点的方向。检测出两个三角面是否一个朝正面，一个朝背面。我们可以再几何着色器的帮助下实现检测过程。</li><li>最后一个种类就是混合了上述的几种渲染方法。例如，首先找到精确的轮廓边，把模型和轮廓边渲染到纹理中，再使用图像处理的方法识别出轮廓线，并在图像空间下进行风格化渲染。<h3 id="添加高光"><a href="#添加高光" class="headerlink" title="添加高光"></a>添加高光</h3></li><li>计算normal和halfDir的点乘结果，把该值和一个阈值进行比较，如果小于该阈值，高光反射系数为0，否则返回1；<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">float spec</span> = dot(worldNormal,worldHalfDir);<br><span class="hljs-attribute">spec</span> = step(threshold,spec);<br></code></pre></td></tr></table></figure></li><li>系数直接突变会使边界处产生锯齿，我们可以在边界处很小的一块区域内，进行平滑处理。<figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">float spec</span> = dot(worldNormal,worldHalfDir);<br><span class="hljs-attribute">spec</span> = smoothstep(-w,w,spec - threshold);<br></code></pre></td></tr></table></figure></li></ul><h2 id="像素风格的渲染"><a href="#像素风格的渲染" class="headerlink" title="像素风格的渲染"></a>像素风格的渲染</h2><ul><li>使用了提前生成的素描纹理来实现实时的素描风格渲染，这些纹理组成了一个<strong>色调艺术映射</strong><br><img src="https://img-blog.csdnimg.cn/20190507110327608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDg2NjM5,size_16,color_FFFFFF,t_70" alt="TAM例子">  </li><li>图中从左到右纹理中的笔触逐渐增多，用于模拟不同光照下的漫反射效果。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86%20%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="高级纹理"><a href="#高级纹理" class="headerlink" title="高级纹理"></a>高级纹理</h1><h2 id="立方体纹理"><a href="#立方体纹理" class="headerlink" title="立方体纹理"></a>立方体纹理</h2><ul><li><strong>环境映射</strong>的一种实现方式。</li><li>包含六张图像，对应立方体六个面。</li><li>纹理采样需要提供一个三维坐标，坐标作为一个矢量从立方体中心出发，采样结果就是矢量与立方体表面交点的信息。</li><li>优点：实现简单，得到的效果好。</li><li>缺点：引入新的物体、光源，或者物体发生移动时，需要重新生成。</li><li>常用与天空盒以及环境映射。<h3 id="天空盒"><a href="#天空盒" class="headerlink" title="天空盒"></a>天空盒</h3></li><li>模拟天空或室内等背景</li><li>Unity中，天空盒是在所有不透明物体之后渲染的。<h3 id="用于环境映射的立方体纹理"><a href="#用于环境映射的立方体纹理" class="headerlink" title="用于环境映射的立方体纹理"></a>用于环境映射的立方体纹理</h3></li><li>Unity5中创建用于环境映射的立方体纹理的方法：</li></ul><ol><li>直接由特殊布局的纹理生成<br>需要一张具有特殊布局，类似立方体展开的交叉布局、全景布局。把纹理的Texture Type设置为Cubemap即可。</li><li>手动创建一个Cubemap资源，再给它添加6张图<br>与天空盒材质创建相同。Unity5后建议使用第一种方式，方便压缩、边缘修正、光滑反射和HDR</li><li>由脚本生成。<br>前两种方法需要提供提前准备好的纹理图像。Unity中可以使用<strong>Camera.RenderToCubemap</strong>函数把从任意位置观察到的图像存储到6张图像中，从而创建出对应位置上的立方体纹理。</li></ol><h4 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h4><ul><li>因为光线可逆，我们通过视角方向（指向表面）和表面法线可以计算出入射光线的方向，通过这个方向对纹理进行采样就可以得到反射信息。</li></ul><h4 id="折射"><a href="#折射" class="headerlink" title="折射"></a>折射</h4><ul><li>当光线从一种介质斜射入另一种介质时，传播方向一般会发生改变。</li><li>给定入射角时，我们可以使用<strong>斯涅尔定律</strong>来计算反射角。</li><li>光从介质1沿着和表面法线夹角为θ<sub>1</sub>的方向斜射入介质2时，我们可以使用如下公式计算折射光线与法线的夹角θ<sub>2</sub>：</li></ul><p><strong>η<sub>1</sub>sinθ<sub>1</sub>&#x3D;η<sub>2</sub>sinθ<sub>2</sub></strong><br>η<sub>1</sub>、η<sub>2</sub>分别为两个介质的<strong>折射率（index of refraction）</strong><br><img src="https://img-blog.csdnimg.cn/20190426220743848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNDg2NjM5,size_16,color_FFFFFF,t_70" alt="斯涅尔定律图解">  </p><h4 id="菲涅尔反射"><a href="#菲涅尔反射" class="headerlink" title="菲涅尔反射"></a>菲涅尔反射</h4><ul><li>根据视角方向控制反射程度。  </li><li>关于菲涅尔反射，建议看看这个网站：<br><a href="http://gad.qq.com/program/translateview/7195431">万物皆有菲涅尔反射</a>  </li><li>Schilck菲涅尔近似等式：<br>F<sub>schilk</sub>(<strong>v</strong>,<strong>n</strong>)&#x3D;F<sub>0</sub>+(1-F<sub>0</sub>)(1-<strong>v</strong>·<strong>n</strong>)<sup>5</sup><br>其中F<sub>0</sub>为反射系数，用于控制反射强度，<strong>v</strong>是视角方向，<strong>n</strong>是表面法线。</li><li>Empricial 菲涅尔近似等式：<br>F<sub>Empricial</sub>(<strong>v</strong>,<strong>n</strong>)&#x3D;max(0,min(1,bias+scale×(1-<strong>v</strong>·<strong>n</strong>)<sup>power</sup>))<br>其中bias、scale和power都是控制项。</li></ul><h2 id="渲染纹理"><a href="#渲染纹理" class="headerlink" title="渲染纹理"></a>渲染纹理</h2><ul><li>之前学习中，一个摄像机的渲染结果会输出到颜色缓冲中。现在GPU允许我们把整个三维场景渲染到一个中间缓冲中，即<strong>渲染目标纹理（Render Target Texture，RTT）</strong>，而不是传统的帧缓冲或后备缓冲。</li><li>与之相关的是<strong>多冲渲染目标（Multiple Render Target，MRT）</strong>，允许我们把场景同时渲染到多个目标纹理中。</li><li>Unity专门定义了一种纹理类型——<strong>渲染纹理（Render Texture）</strong></li><li>Unity中有两种方式使用渲染纹理：</li></ul><ol><li>在Project目录下创建一个渲染纹理，把某个摄像机的渲染目标设置成该渲染纹理，这样摄像机的渲染结果会实时更新到渲染纹理中，而不会显示在屏幕上。</li><li>屏幕后处理时，使用GrabPass命令或OnRenderImage函数来获取当前屏幕图像，Unity会把这个屏幕图像放到一张和屏幕分辨率等同的渲染纹理中，之后我们可以在自定义的Pass中把它们当做普通的纹理来处理，从而实现各种屏幕特效。</li></ol><h3 id="玻璃效果"><a href="#玻璃效果" class="headerlink" title="玻璃效果"></a>玻璃效果</h3><ul><li>Unity shader中我们可以使用一种特殊的Pass来完成获取屏幕图像的目的，这就是GrabPass。</li><li>在shader中定义一个GrabPass后，Unity会把当前屏幕的截图保存在一张纹理中，我们可以再之后的Pass中访问它。</li><li>使用GrabPass时，需要额外小心<strong>物体的渲染队列设置</strong>。</li><li>我们需要把物体的渲染队列设置为透明队列（”Queue” &#x3D; “Tansparent”）。</li></ul><h2 id="程序纹理"><a href="#程序纹理" class="headerlink" title="程序纹理"></a>程序纹理</h2><ul><li>指的是由计算机生成的图像。</li><li>好处在于可以使用各种参数来控制纹理的外观。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%AD%A6%E4%B9%A0Shader%E6%89%80%E9%9C%80%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <url>/blog/2021/03/31/UnityShader/Unity%20Shader%20%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%AD%A6%E4%B9%A0Shader%E6%89%80%E9%9C%80%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h1 id="学习Shader所需的数学基础"><a href="#学习Shader所需的数学基础" class="headerlink" title="学习Shader所需的数学基础"></a>学习Shader所需的数学基础</h1><h2 id="笛卡尔坐标系"><a href="#笛卡尔坐标系" class="headerlink" title="笛卡尔坐标系"></a>笛卡尔坐标系</h2><h3 id="三维笛卡尔坐标系"><a href="#三维笛卡尔坐标系" class="headerlink" title="三维笛卡尔坐标系"></a>三维笛卡尔坐标系</h3><ul><li>左手坐标系</li><li>右手坐标系<h3 id="Unity使用的坐标系"><a href="#Unity使用的坐标系" class="headerlink" title="Unity使用的坐标系"></a>Unity使用的坐标系</h3></li><li>模型空间 左手坐标系</li><li>世界空间 左手坐标系</li><li>观察空间 右手坐标系，摄像机前向是z轴的负方向。<h2 id="坐标变换"><a href="#坐标变换" class="headerlink" title="坐标变换"></a>坐标变换</h2></li><li>模型空间变换到世界空间中，这个变换叫做模型变换。</li><li>世界空间变换的观察空间，这个变换叫做观察变换。</li><li>观察空间变换到裁剪空间（齐次剪切空间），这个变换的矩阵叫做裁剪矩阵，也被称为投影矩阵。<br>裁剪空间由视锥体决定<br>视锥体定义了场景内的一个坐标空间，只有空间内的物体会被渲染，Camera中的设置影响这个空间。  <h3 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a>透视投影</h3>FOV为视锥体纵向张开角度<br>Near决定近裁剪平面距离相机的距离<br>Far决定远裁剪平面距离相机的距离<br>Viewport Rect中的W，H属性共同决定纵横比<br>变换后若一点在空间内，则：</li></ul><p>-w &lt;&#x3D; x &lt;&#x3D; w<br>-w &lt;&#x3D; y &lt;&#x3D; w<br>-w &lt;&#x3D; z &lt;&#x3D; w</p><h3 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h3><p>Size决定视锥体竖直方向上高度的一半<br>Near决定近裁剪平面距离相机的距离<br>Far决定远裁剪平面距离相机的距离  </p><ul><li>视锥体投影到屏幕空间中，我们便会的得到真正的像素位置，而不是虚拟的三维坐标<br>我们需要先进行齐次除法：用w分量去处理x,y,z分量，得到归一化的设备坐标。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
